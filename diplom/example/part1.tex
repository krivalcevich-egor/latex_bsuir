%------------------------------------------------------------------------------
% \PART 1
%------------------------------------------------------------------------------
\chapter[Обзор существующих нейронных сетей для распознавания рукописных цифр]
{ОБЗОР СУЩЕСТВУЮЩИХ НЕЙРОННЫХ СЕТЕЙ ДЛЯ РАСПОЗНАВАНИЯ РУКОПИСНЫХ ЦИФР}

%------------------------------------------------------------------------------
% \PART 1.1 Text
%------------------------------------------------------------------------------
\section{Существующие нейронные сети для распознавания рукописных цифр}\par
\hspace*{12.5 mm}Распознавание рукописных цифр является одной из классических 
задач машинного обучения, для решения которой применяется широкий спектр 
нейронных сетей. В зависимости от сложности задачи, требований к точности, 
скорости обработки и аппаратных ограничений используются различные архитектуры, 
включая полносвязные сети, сверточные нейронные сети и более сложные гибридные 
модели.

Одним из первых методов распознавания рукописных цифр стали многослойные 
перцептроны (MLP), относящиеся к классу полносвязных нейронных сетей (FCNN). 
Такие сети состоят из входного слоя, нескольких скрытых слоев и выходного слоя, 
где каждый нейрон соединен со всеми нейронами предыдущего и последующего слоев. 
Несмотря на их простоту, они способны успешно решать задачу распознавания, но 
требуют большого количества параметров и вычислительных ресурсов, что делает 
их менее эффективными по сравнению с более специализированными архитектурами.

Для обработки изображений, включая рукописные цифры, более эффективными 
оказались сверточные нейронные сети (CNN). Эти сети включают сверточные слои, 
которые извлекают характерные признаки из изображения, а также слои 
субдискретизации для уменьшения размерности. CNN значительно превосходят 
полносвязные сети в точности и скорости работы, поскольку используют 
пространственные зависимости в данных и требуют меньше параметров.

Хотя рекуррентные нейронные сети (RNN) и их усовершенствованные версии, такие 
как Long Short-Term Memory Network (LSTM) и Gated Recurrent Unit Network (GRU),
чаще применяются для обработки последовательных данных, они могут 
использоваться и для распознавания рукописных цифр. В частности, они 
эффективны в случаях, когда рукописные символы анализируются в контексте 
строки, например, при распознавании рукописного текста.

Современные архитектуры, такие как Vision Transformer (ViT) и его модификации, 
предлагают альтернативный подход к обработке изображений, основанный на 
механизме самовнимания. Хотя традиционно трансформеры использовались в 
обработке естественного языка (NLP), их успешное применение в компьютерном 
зрении позволило достичь новых высот в распознавании символов и цифр.

Для повышения точности и эффективности могут применяться гибридные подходы, 
комбинирующие преимущества разных типов нейронных сетей. Например, модели, 
совмещающие CNN и LSTM, успешно применяются для распознавания рукописного 
текста, где CNN используется для выделения признаков, а LSTM — для анализа 
последовательностей.

На сегодняшний день существует множество архитектур нейронных сетей, способных 
эффективно решать задачу распознавания рукописных цифр. Выбор конкретного 
метода зависит от требований к точности, вычислительным ресурсам и целевой 
платформе. В последующих разделах будет представлен детальный анализ наиболее 
распространенных нейросетевых моделей, применяемых для данной задачи.

%------------------------------------------------------------------------------
% \PART 1.2 Text
%------------------------------------------------------------------------------
\section{Полносвязные нейронные сети}\par
\hspace*{12.5 mm}Полносвязная нейронная сеть (Fully Connected Neural Network) — 
это базовая архитектура искусственных нейронных сетей, в которой каждый нейрон 
одного слоя соединен со всеми нейронами следующего слоя. Такой тип архитектуры 
используется для различных задач, включая распознавание рукописных цифр, но 
чаще всего применяется в качестве классификатора после сверточных или 
рекуррентных слоев. Количество нейронов на каждом слое может отличаться от 
соседнихслоев.

%------------------------------------------------------------------------------
% \PART 1.2.1 Text
%------------------------------------------------------------------------------
\subsection{Структура полносвязного слоя}\par
\hspace*{12.5 mm}Полносвязная нейронная сеть состоит из следующих основных 
компонентов:

    1 \text{Входной слой} — принимает данные (например, изображение 
    $28\times28$ пикселей в задаче распознавания рукописных цифр, развёрнутое в
    вектор размером 784).

    2 \text{Скрытые слои} — выполняют нелинейные преобразования входных данных. 
    Обычно включают несколько таких слоев.

    3 \text{Выходной слой} — содержит количество нейронов, соответствующее 
    числу классов (например, 10 для цифр 0–9), и использует функцию активации, 
    например softmax. Функция активации — это математическая функция, которая 
    определяет, передаст ли нейрон сигнал дальше по сети.

Общая структура полносвязных нейронных сетей показана на 
рисунке~\ref{fig:fcnn}.

Математически данную сеть можно описать формулой для вычисления выхода нейрона 
в полносвязном слое:

\begin{equation}
    {h} = f({W} {x} + {b})
\end{equation}

\noindentгде: ${x}$ — входной вектор размерности $d$,
              ${W}$ — матрица весов размерности $n \times d$,
              ${b}$ — вектор смещений размерности $n$,
              $f(\cdot)$ — функция активации (например, ReLU, сигмоида),
              ${h}$ — выходной вектор нейронов текущего слоя.

Для многослойной архитектуры выход слоя $l$ является входом для следующего слоя, 
что можно описать уравнением:

\begin{equation}
    {h}^{(l+1)} = f({W}^{(l)} {h}^{(l)} + {b}^{(l)})
\end{equation}

\insertfigure{fcnn}{pic/fcnn.png}{Общая структура полносвязных нейронных сетей}{13.5cm}

%------------------------------------------------------------------------------
% \PART 1.2.1 Text
%------------------------------------------------------------------------------
\subsection{Функция активации ReLU}
\hspace*{12.5 mm}ReLU (Rectified Linear Unit) — одна из наиболее популярных и 
широко используемых функций активации в нейронных сетях. Она определяется 
следующим образом:

\begin{equation}
    f(x) = \max(0, x)
\end{equation}

График функции ReLU представлен на рисунке~\ref{fig:relu}.

\insertfigure{relu}{pic/ReLU.png}{График функции ReLU}{7cm}

Функция ReLU ускоряет обучение сети и устранят проблему исчезающего градиента, 
так как имеет линейное поведение для положительных значений. Сама функция 
проста в реализации, но может быть чувствительна к большим значениям градиентов
и в отрицательной области возвращает 0.

%------------------------------------------------------------------------------
% \PART 1.2.2 Text
%------------------------------------------------------------------------------
\subsection{Функция активации сигмоида}
\hspace*{12.5 mm}Сигмоида (Logistic Function) — это одна из классических 
функций активации, используемая в нейронных сетях. Её математическое 
определение:

\begin{equation}
    f(x) = \frac{1}{1 + e^{-x}}
\end{equation}\\[-9mm]

График функции сигмоида представлен на рисунке~\ref{fig:sigmoid}.

\insertfigure{sigmoid}{pic/sigmoid.png}{График функции сигмоида}{7cm}

Сигмоида подходит для моделирования вероятностей, так как имеет диапазон 
значений $ (0,1) $. Функция гладкая и дифференцируемая, что упрощает вычисление
градиента, однако выходы не центрированы вокруг нуля, что замедляет обучение.

%------------------------------------------------------------------------------
% \PART 1.2.3 Text
%------------------------------------------------------------------------------
\subsection{Функция активации Softmax}
\hspace*{12.5 mm}Функция Softmax применяется в многоклассовой классификации и 
преобразует входные значения в вероятностное распределение:

\begin{equation}
    \sigma{(z_i)} = \frac{e^{z_i}}{\sum_{j} e^{z_j}},
\end{equation}

Гистограмма функции Softmax представлен на рисунке~\ref{fig:sm}.

\insertfigure{sm}{pic/softmax.png}{Гистограмма функции Softmax}{7cm}

\noindentгде $z_i$ — входное значение для $i$-го класса, а знаменатель 
представляет собой сумму экспонент всех входных значений.

Softmax позволяет интерпретировать выходные значения сети как вероятности 
принадлежности к классам. Однако склонна к затуханию градиента при слишком 
больших входных значениях.

%------------------------------------------------------------------------------
% \PART 1.2.4 Text
%------------------------------------------------------------------------------
\subsection{Функция активации гиперболический тангенс}
\hspace*{12.5 mm}Гиперболический тангенс (Tanh) — это сигмоидная функция, но 
симметричная относительно начала координат. Он определяется следующим образом:

\begin{equation}
    f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\end{equation}

Функция принимает значения в диапазоне $(-1, 1)$, что делает её более 
предпочтительной по сравнению с сигмоидой в скрытых слоях нейронных сетей. 
Основные преимущества функции $\tanh(x)$:

1 Среднее значение её выходов ближе к нулю, что помогает ускорить сходимость 
сети.

2 Производная функции принимает большие значения в среднем диапазоне, что 
уменьшает проблему исчезающего градиента по сравнению с сигмоидой.

3 Хорошо подходит для задач, где требуется сбалансированный выход между 
положительными и отрицательными значениями.

График функции гиперболический тангенс представлен на рисунке~\ref{fig:tang}.

\insertfigure{tang}{pic/tang.png}{График функции гиперболический тангенс}{7cm}

%------------------------------------------------------------------------------
% \PART 1.2.5 Text
%------------------------------------------------------------------------------
\subsection{Обучение сети}
\hspace*{12.5 mm}Обучение FCNN осуществляется с использованием метода обратного 
распространения ошибки (Backpropagation) и оптимизационного алгоритма, 
например, стохастического градиентного спуска (SGD):

\begin{equation}
    W^{(l)} \leftarrow W^{(l)} - \eta \frac{\partial L}{\partial W^{(l)}}
\end{equation}

\begin{equation}
    b^{(l)} \leftarrow b^{(l)} - \eta \frac{\partial L}{\partial b^{(l)}}
\end{equation}

\noindentгде $\eta$ — скорость обучения, а $L$ — функция потерь, например, 
кросс-энтропия для классификации:

\begin{equation}
    L = - \sum_{i} y_i \log(\hat{y}_i)
\end{equation}

\noindentгде \( y_i \) – истинное значение, а \( \hat{y_i} \) – предсказанное 
значение.

Энтропия – измеряет степень неопределенности распределения вероятностей. А 
кросс-энтропия измеряет, насколько вероятностное распределение, предсказанное 
моделью, отличается от истинного распределения.

%------------------------------------------------------------------------------
% \PART 1.2.6 Text
%------------------------------------------------------------------------------
\subsection{Анализ полносвязной нейрооной сети}
\hspace*{12.5 mm}Структура однослойной нейронной сети прямого распространения, 
состоящей из полносвязного слоя с выходной функцией активации softmax показана 
на рисунке~\ref{fig:nn-struct}.

\insertfigure{nn-struct}{pic/NN_struct.png}{Однослойной нейронной сети прямого распространения}{10cm}

Данная архитектура позволяет добиться точности 92,4\%\cite{Doc}. Нейронная сеть 
использует 8624 параметра, которые необходимо хранить в памяти в качестве 
весовых коэффициентов и смещений.

%------------------------------------------------------------------------------
% \PART 1.3 Text
%------------------------------------------------------------------------------
\section{Сверточные нейронные сети}
\hspace*{12.5 mm}
Сверточные нейронные сети (Convolutional Neural Networks) представляют собой 
архитектуру глубокого обучения, предназначенную для обработки данных обладающих 
сетчатой топологией, таких как изображения. Их основное преимущество 
заключается в способности эффективно извлекать пространственные и иерархические 
особенности входных данных с помощью операций свертки.

Принцип работы сверточных нейронных сетей показан на рисунке~\ref{fig:cnn}\cite{CNN}.

\insertfigure{cnn}{pic/cnn.png}{Принцип работы сверточных нейронных сетей}{13cm}

%------------------------------------------------------------------------------
% \PART 1.3.1 Text
%------------------------------------------------------------------------------
\subsection{Основные компоненты сверточной нейронной сети}
\hspace*{12.5 mm}Стандартная архитектура CNN включает в себя следующие основные 
слои:

    1 \text{Сверточные слои} – выполняют операцию свертки, выделяя значимые 
признаки входных данных.

    2 \text{Функция активации} – вводит нелинейность, наиболее часто 
используется ReLU.\@

    3 \text{Слои подвыборки (Pooling)} – уменьшают размерность представления и 
повышают устойчивость к небольшим сдвигам.

    4 \text{Полносвязные слои} – выполняют классификацию извлеченных признаков.

    5 \text{Функция потерь} – используется для оценки ошибки модели, например, 
кросс-энтропия.

%------------------------------------------------------------------------------
% \PART 1.3.2 Text
%------------------------------------------------------------------------------
\subsection{Операция свертки}
\hspace*{12.5 mm}Основная операция в сверточных нейронных сетях – это свертка, 
которая формально определяется следующим образом:

\begin{equation}
    S(i, j) = \sum_{m} \sum_{n} X(i+m, j+n) \cdot K(m, n),
\end{equation}

\noindentгде:
    \( X(i, j) \) – входное изображение,
    \( K(m, n) \) – ядро свертки,
    \( S(i, j) \) – выходное изображение после свертки.

Сверточный слой применяется к входному изображению, используя несколько 
фильтров, каждый из которых извлекает определенные характеристики.

%------------------------------------------------------------------------------
% \PART 1.3.3 Text
%------------------------------------------------------------------------------
\subsection{Слои подвыборки}
\hspace*{12.5 mm}Для уменьшения размерности карт признаков применяется операция 
подвыборки. Один из наиболее распространенных методов – \text{max pooling}, 
который выбирает максимальное значение в окне \( k \times k \):

\begin{equation}
    P(i, j) = \max_{(m, n) \in k \times k} S(i+m, j+n).
\end{equation}

Этот метод помогает уменьшить объем вычислений и делает модель более устойчивой
к сдвигам изображения.

%------------------------------------------------------------------------------
% \PART 1.3.4 Text
%------------------------------------------------------------------------------
\subsection{Обучение сверточной нейронной сети}
\hspace*{12.5 mm}Обучение CNN происходит с использованием алгоритма обратного 
распространения ошибки и градиентного спуска. Для обновления параметров 
используется правило:

\begin{equation}
    w^{(t+1)} = w^{(t)} - \eta \frac{\partial L}{\partial w},
\end{equation}

\noindentгде \( \eta \) – коэффициент обучения, а \( \frac{\partial L}{\partial w} \) – 
градиент функции потерь по параметру \( w \).

%------------------------------------------------------------------------------
% \PART 1.3.5 Text
%------------------------------------------------------------------------------
\subsection{Анализ сверточной нейрооной сети}
\hspace*{12.5 mm}Структура сверточной нейронной сети, показатели точности 
которой 90\% показана на рисунке~\ref{fig:cnn-struct}\cite{CNN}.

\insertfigure{cnn-struct}{pic/CNN_STRUCT.png}{Однослойной нейронной сети прямого распространения}{10cm}

Данная конфигурации состоит из: 

  1 Входной слой: [28$\times$28]

  2 Сверточный слой: 3 маски [3$\times$3] 

  3 Слой пакетной нормализации 

  4 Слой ReLU 

  5 Полносвязанный слой 

  6 Слой Softmax 

  7 Слой классификации

Нейронная сеть использует 23520 параметра, которые необходимо хранить в памяти 
в качестве весовых коэффициентов и смещений.

%------------------------------------------------------------------------------
% \PART 1.4 Text
%------------------------------------------------------------------------------
\section{Рекуррентные нейронные сети}
\hspace*{12.5 mm}Рекуррентные нейронные сети (Recurrent Neural Networks) 
предназначены для обработки последовательных данных, таких как текст, 
аудиозаписи, временные ряды и рукописные символы. В отличие от полносвязных и 
сверточных сетей, рекуррентные обладают внутренним состоянием, позволяющим 
учитывать информацию из предыдущих шагов при обработке текущего входного 
сигнала.

Принцип работы рекуррентных нейронных сетей показан на рисунке~\ref{fig:rec}.

\insertfigure{rec}{pic/rec.png}{Принцип работы рекуррентных нейронных сетей}{11cm}

%------------------------------------------------------------------------------
% \PART 1.4.1 Text
%------------------------------------------------------------------------------
\subsection{Основная идея рекуррентных нейронных сетей}
\hspace*{12.5 mm}Основное отличие RNN от других типов нейронных сетей 
заключается в наличии обратных связей, позволяющих передавать информацию от 
предыдущих состояний. Для каждого временного шага \( t \) вычисляется новое 
состояние \( h_t \) на основе входных данных \( x_t \) и предыдущего состояния 
\( h_{t-1} \):

\begin{equation}
    h_t = f(W_x x_t + W_h h_{t-1} + b),
\end{equation}

\noindentгде:
    \( h_t \) — скрытое состояние в момент времени \( t \);
    \( x_t \) — входной вектор в момент времени \( t \);
    \( W_x \), \( W_h \) — обучаемые весовые матрицы;
    \( b \) — вектор смещения;
    \( f \) — функция активации, например \( \tanh \) или \( ReLU \).

Выход сети \( y_t \) определяется как:

\begin{equation}
    y_t = g(W_y h_t + c),
\end{equation}

\noindentгде \( W_y \) — матрица весов выходного слоя, 
\( c \) — вектор смещения, 
а \( g \) — функция активации выходного слоя.

%------------------------------------------------------------------------------
% \PART 1.4.2 Text
%------------------------------------------------------------------------------
\subsection{Проблемы стандартных рекуррентных нейронных сетей}
\hspace*{12.5 mm}Обычные RNN сталкиваются с проблемой затухающих и взрывающихся
градиентов при обучении, что делает сложным обучение долгосрочных зависимостей. 
Для решения этих проблем были разработаны улучшенные архитектуры, такие как:
долгая краткосрочная память (Long Short-Term Memory) и управляемые рекуррентные 
блоки (Gated Recurrent Unit).

Эти архитектуры используют специальные механизмы управления потоками 
информации, такие как входные, выходные и забывающие ворота, позволяя 
эффективно обрабатывать длинные последовательности данных.

%------------------------------------------------------------------------------
% \PART 1.4.3 Text
%------------------------------------------------------------------------------
\subsection{Применение RNN в распознавании рукописных цифр}
\hspace*{12.5 mm}Рекуррентные сети могут применяться для распознавания 
рукописных цифр, особенно в задачах последовательного анализа. Один из 
распространенных подходов — обработка строк рукописного текста, где каждый 
символ представлен как последовательность пикселей или векторных признаков. В 
таких задачах используются LSTM или GRU, способные учитывать 
пространственно-временные зависимости между элементами изображения. Пример 
архитектуры LSTM представлен на рисунке~\ref{fig:rnn-struct}\cite{LST2D}.

\insertfigure{rnn-struct}{pic/rnn_struct.png}{Пример архитектуры LSTM}{9cm}

Данная архитектура позволяет решать задачу распознавания рукописного текста с 
точностью 94,7\%.

%------------------------------------------------------------------------------
% \PART 1.5 Text
%------------------------------------------------------------------------------
\section{Трансформеры нейронные сети}\par
\hspace*{12.5 mm}Трансформеры представляют собой архитектуру нейронных сетей, 
которая достигла значительных успехов в обработке последовательностей данных. В
отличие от рекуррентных нейронных сетей, трансформеры используют механизмы 
самовнимания (self-attention) для обработки входной информации параллельно, что 
позволяет достигать высокой эффективности и точности.

%------------------------------------------------------------------------------
% \PART 1.5.1 Text
%------------------------------------------------------------------------------
\subsection{Архитектура трансформера}
\hspace*{12.5 mm}Архитектура трансформера состоит из энкодера и декодера, 
каждый из которых содержит несколько слоев. Основные компоненты трансформера:

  1 \text{Механизм самовнимания (self-attention)} — позволяет модели учитывать 
важность различных частей входных данных.

  2 \text{Многоголовочное внимание (multi-head attention)} — улучшает 
способность модели учитывать разные аспекты входной информации.

  3 \text{Обратная связь (residual connections)} — ускоряет обучение и 
предотвращает исчезновение градиента.

  4 \text{Механизм нормализации (layer normalization)} — стабилизирует 
обучение.

  5 \text{Feed-forward сети} — полносвязные слои, применяемые после механизма 
самовнимания.

Механизм самовнимания вычисляется следующим образом:

\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V,
\end{equation}

\noindentгде $Q$ (query), $K$ (key) и $V$ (value) — матрицы, полученные линейными 
преобразованиями входных данных, а $d_k$ — размерность ключей.

%------------------------------------------------------------------------------
% \PART 1.5.2 Text
%------------------------------------------------------------------------------
\subsection{Преимущества трансформеров}
\hspace*{12.5 mm}В отличие от RNN, трансформеры обрабатывают входные данные 
одновременно, что ускоряет обучение. Механизм самовнимания позволяет эффективно 
учитывать контекст на больших расстояниях. Трансформеры могут применяться к 
различным задачам, включая обработку текста, изображений и речи.

%------------------------------------------------------------------------------
% \PART 1.5.3 Text
%------------------------------------------------------------------------------
\subsection{Применение трансформеров в распознавании рукописных цифр}
\hspace*{12.5 mm}Хотя трансформеры изначально были разработаны для задач 
обработки естественного языка, они также успешно применяются для распознавания 
рукописных цифр. Основной подход заключается в преобразование изображений в 
последовательности пикселей и обработка их с помощью Vision Transformer (ViT).

Трансформеры обеспечивают высокую точность  – 90,30\% и используют большое 
количество параметров – 163729 при обработке изображений, особенно при наличии 
больших объемов обучающих данных, что делает их перспективным направлением для 
распознавания рукописных цифр\cite{Agrawal}.

%------------------------------------------------------------------------------
% \PART 1.6 Text
%------------------------------------------------------------------------------
\section{Гибридные архитектуры}\par
\hspace*{12.5 mm}Гибридные архитектуры нейронных сетей представляют собой 
комбинацию различных типов нейронных сетей, объединяя их преимущества для 
улучшения производительности в задачах распознавания изображений, в том числе 
рукописных цифр. Такие модели могут включать элементы сверточных, рекуррентных, 
трансформерных и полносвязных сетей.

%------------------------------------------------------------------------------
% \PART 1.6.1 Text
%------------------------------------------------------------------------------
\subsection{Комбинация CNN и RNN}
\hspace*{12.5 mm}Одним из наиболее распространенных гибридных подходов является 
использование сверточных нейронных сетей для извлечения признаков и 
рекуррентных нейронных сетей для обработки последовательностей. 

CNN выполняет обработку изображений, извлекая пространственные признаки, 
которые затем передаются в RNN.\@

RNN, такие как LSTM или GRU, анализируют временные зависимости между 
последовательными фрагментами извлеченных признаков, что особенно полезно 
при обработке последовательностей символов или цифр.

Математически процесс можно описать следующим образом:

\begin{equation}
    F = \text{CNN}(X)
\end{equation}
\begin{equation}
    h_t = \text{RNN}(F_t, h_{t-1})
\end{equation}

\noindentгде $X$ – входное изображение, $F$ – извлеченные признаки, $h_t$ – скрытое 
состояние RNN.\@

%------------------------------------------------------------------------------
% \PART 1.6.2 Text
%------------------------------------------------------------------------------
\subsection{Сеть CNN-Transformer}
\hspace*{12.5 mm}Другой популярный гибридный подход – объединение CNN и 
трансформеров. CNN используются для локального извлечения признаков, а механизм
внимания трансформера помогает учитывать глобальный контекст изображения.

CNN генерирует карту признаков изображения.

Трансформер обрабатывает полученные признаки, используя механизм самовнимания 
(Self-Attention).

Формально, этот процесс можно записать следующим образом:

\begin{equation}
    F = \text{CNN}(X)
\end{equation}
\begin{equation}
    Z = \text{Transformer}(F)
\end{equation}

\noindentгде $Z$ – закодированное представление входного изображения, учитывающее 
пространственные и контекстные зависимости.

%------------------------------------------------------------------------------
% \PART 1.6.3 Text
%------------------------------------------------------------------------------
\subsection{Применение гибридных архитектур в распознавании рукописных цифр}
\hspace*{12.5 mm}
Гибридные модели активно применяются для задач распознавания рукописных цифр, 
поскольку они позволяют достичь высокой точности за счет сочетания мощных 
методов обработки изображений и анализа последовательностей. 

    CNN-RNN используются для распознавания рукописных цифр в 
последовательностях, таких как почтовые индексы.

    CNN-Transformer подходят для высокоточного классифицирования цифр в 
условиях зашумленных или искаженных изображений. Сеть, архитектура которой 
представлена на рисунке~\ref{fig:CNN-transformer}, позволяет добиться 
точности 99,89\%\cite{Performance}.

\insertfigure{CNN-transformer}{pic/cnn_transformer.jpeg}{Архитектура CNN-Transformer}{16cm}

Благодаря комбинированию разных методов гибридные архитектуры обеспечивают 
улучшенные результаты в задачах компьютерного зрения.