%------------------------------------------------------------------------------
% \PART 3
%------------------------------------------------------------------------------
\chapter[Разработка структуры IP-блока нейронной сети для распознавания рукописных цифр]
{РАЗРАБОТКА СТРУКТУРЫ IP-БЛОКА НЕЙРОННОЙ СЕТИ ДЛЯ РАСПОЗНАВАНИЯ РУКОПИСНЫХ ЦИФР}

%------------------------------------------------------------------------------
% \PART 3.1 Text
%------------------------------------------------------------------------------
\section{Функциональная спецификация системы}\par
\hspace*{12.5 mm}Для разработки системы распознавания рукописных цифр с 
использованием нейронной сети прямого распространения необходимо составить 
функциональную спецификацию, которая будет описывать требования к системе, её 
функциональные компоненты и интерфейсы с пользователем и окружением.


Функциональная спецификация системы распознавания рукописных цифр должна 
включать два основных компонента:

  1 Список функций, выполняемых системой.
    
  2 Описание интерфейса между системой и пользователем.

Система будет реализована как IP-блок нейронной сети прямого распространения, 
которая предварительно будет обучаться на наборе данных изображений рукописных 
цифр, а затем использоваться для их распознавания.

В контексте требований пользователя система должна отвечать на следующие 
вопросы:

  1 Какие средства необходимо предусмотреть для ввода данных?

  2 Как будет происходить обучение модели и её оценка?

  3 Какие средства необходимо предусмотреть для вывода результатов?

  4 Как будет осуществляться хранение весовых коэффициентов?

  5 Какие средства необходимы для работы с пользователем?

Ответив на эти вопросы, можно составить функциональную спецификацию для 
разрабатываемой системы.

  1 Данные о рукописных цифрах будут передаваться в виде последовательной 
передачи пикселей изображений, которые будут поступать в IP-блок по AXI4-lite 
интерфейсу из процессорной системы на ПЛИС.
    
  2 Для обучения модели будет использоваться набор данных MNIST, который 
разделен на тренировочные и тестовые данные. Оценка качества модели будет 
проводиться с помощью метрик точности и потерь. 
    
  3 Результаты работы модели будут поступать в процессорную систему по 
интерфейсу AXI4-lite, где проходить обработку, например использоватья в 
построении матрицы ошибок.
    
  4 Хранение весовых коэффициентов будет осуществляться в блочной памяти 
(BRAM). Необходимо обеспечить поддержку трех весовых групп: первого слоя LST, 
второго слоя LST и выходного полносвязного слоя.
    
  5 Система будет предоставлять IP-блок, который будет подключаться по 
AXI4-lite интерфейсу к другим блокам системы.

%------------------------------------------------------------------------------
% \PART 3.2
%------------------------------------------------------------------------------
\section{Разбиение системы на модули}\par
\hspace*{12.5 mm}Система распознавания рукописных цифр на базе нейронной сети 
прямого распространения может быть разделена на несколько функциональных 
модулей:

  1 Модуль памяти изображения, который хранит входное изображение и все 
промежуточные. 

  2 Модуль обработки пикселя, который передает пиксель и необходимый весовой 
коэффициент в модуль выполнения операции MAC.\@
  
  3 Модуль MAC выполняет операцию умножения с накоплением входных данных.

  4 Модули памяти весов, в которых хранятся весовые коэффициенты каждого слоя 
нейронной сети.

  5 Модуль функции активации тангенса, который выполняет вычисление функции 
тангенс в LST слое.

  6 Модуль функции активации softmax, который производит выбор результата 
распознавания.

  7 Модуль управления, который осуществляет контроль над расчетом слоев 
нейронной сети.

%------------------------------------------------------------------------------
% \PART 3.3
%------------------------------------------------------------------------------
\section{Выбор соотношения между аппаратными и программными средствами}\par
\hspace*{12.5 mm}Для эффективного функционирования системы важно правильно 
выбрать соотношение между аппаратными и программными средствами.

Основу аппаратных средств для реализации IP-блока нейронной сети прямого 
распространения для распознавания рукописных цифр является ПЛИС Zybo Z7: 
Zynq-7000.

Программные средства, в свою очередь включают в себя все модули, описанные 
ранее а также фреймворк PyTorch, который используется для обучения нейронной 
сети и fixpoint для описанияэталонной модели.

%------------------------------------------------------------------------------
% \PART 3.4
%------------------------------------------------------------------------------
\section{Описание структурной схемы}\par
\hspace*{12.5 mm}Для описания структуры IP-блока нейронной сети прямого 
распространения необходимо показать принцип работы LST-слоя.

%------------------------------------------------------------------------------
% \PART 3.4.1
%------------------------------------------------------------------------------
\subsection{Принцип работы LST преобразования}\par
\hspace*{12.5 mm}Двумерное разделимое преобразование используется в обработке 
изображений для уменьшения вычислительной сложности. Основная идея заключается 
в том, что вместо хранения полного двумерного ядра свёртки размером 
n\(\times\)n используется его представление в виде произведения двух одномерных 
векторов. Это позволяет сократить число параметров с \(n^2\) до \(2n\), что 
снижает затраты на вычисления и память.

\begin{equation}
    W = V \times h^T,
\end{equation}

\noindentгде \(W\in \mathbb{R}^{n\times n}\), \(v, h^T\in \mathbb{R}^{n\times 1}\)

LST основан на идее совместного использования весов одного полностью связанного 
слоя для обработки всех строк изображения. После этого второй общий 
полносвязный слой используется для обработки всех столбцов представления 
изображения, полученных из первого слоя. Использование слоев LST в архитектуре 
NN значительно сокращает количество параметров модели по сравнению с моделями, 
которые используют стекированные полносвязные слои.

LST можно использовать для построения архитектур глубоких нейронных сетей 
(DNN), заменяя традиционную нейронную сеть прямого распространения. Нейронная 
сеть на основе LST позволяет достичь высокой точности, при этом количество 
параметров модели значительно меньше, чем в полносвязной нейронной сети.

Предложенное обученное двумерное разделяемое преобразование (LST2D) 
обрабатывает изображение построчно, а не преобразует его в вектор, как это 
обычно делается в полносвязных сетях. 

С математической точки зрения LST2D принимает двумерное изображение X размером 
din\(\times\)din в качестве входных данных и создает двумерное выходное 
изображение Y размером dout\(\times\)dout:

\begin{equation}
  Y = LST_{d_{in}xd_{out}}(X) = \tan(W_2\tan(W_1X^T))
\end{equation}

\noindentгде W1, W2 — матрицы весов слоев FC1 и FC2 соответственно 
(рисунок~\ref{fig:lst2d-pic})\cite{LST2D}. 

\insertfigure{lst2d-pic}{pic/LST2D.png}{Принцип работы LST2D}{16cm}

Здесь din и dout — гиперпараметры преобразования, определяющие количество 
обучаемых параметров, равное N=2 $\cdot$ (din+1) $\cdot$ dout. 

Простейшая архитектура нейронной сети на основе LST2D для распознавания цифр 
MNIST требует одного блока LST2D, за которым следует полносвязный слой с 
функцией активации softmax. Эта архитектура представлена на 
рисунке~\ref{fig:lst-pic}\cite{LST2D}. 

Структурная схема LST-1 преобразования представлена в приложении Б.

Алгоритм работы LST-1 преобразования представлен в приложении В.

Данную архитектуру можно масштабировать. Пример двухблочной архитектуры LST2D 
приведен на рисунке~\ref{fig:lst2-pic}\cite{LST2D}.

\insertfigure{lst-pic}{pic/LST.png}{Принцип работы LST-1}{10cm}

Модель LST-2 имеет дополнительный параметр dh, который определяет размерность 
dh\(\times\)dh скрытого представления входного изображения.

\insertfigure{lst2-pic}{pic/LST2.png}{Принцип работы LST-2}{11cm}

Пример обработки изображения нейронной сетью архитектуры LST-1 представлен на 
рисунке~\ref{fig:img-proc}\cite{LST2D}.

\insertfigure{img-proc}{pic/img_proc.png}{Архитектура CNN-Transformer}{16cm}

%------------------------------------------------------------------------------
% \PART 3.4.2
%------------------------------------------------------------------------------
\subsection{Принцип работы нейронной сети прямого распространения}\par
\hspace*{12.5 mm}Принцип работы данной нейронной сети основан на комбинации 
обучаемого двумерного разделимого преобразования и полносвязной нейронной сети. 
Такая архитектура позволяет эффективно обрабатывать изображения, снижая 
вычислительные затраты при аппаратной реализации.

Для окончательной классификации используется полносвязный слой с функцией 
активации {softmax}. Данный слой преобразует выходные значения в вероятностное 
распределение по классам, позволяя определить, к какой цифре относится входное 
изображение.

Вычислительное ядро разработанного устройства состоит из 28 MAC 
(Multiply-ACcumulate) ядер, распределенных по специализированным блокам. Эти 
блоки организованы таким образом, чтобы обеспечить эффективное выполнение 
операций умножения входного вектора изображения на матрицу весов слоя
нейронной сети.

Десять MAC-ядер из общего массива используются на финальном тапе вычислений и 
непосредственно участвуют в формировании входных данных для функции активации 
softmax. Эти ядра получают входные данные из трех независимых блоков памяти, 
что позволяет оптимизировать процесс выборки весов и повысить скорость 
вычислений. Остальные 18 MAC-ядер распределены по блокам, где каждая 
вычислительная единица получает данные из двух запоминающих устройств.

IP-блок устройства предназначен для интеграции в систему на кристалле (СнК) и 
взаимодействует с процессорной системой через uP-интерфейс. СнК – это 
интегральная схема, которая объединяет в одном чипе все основные компоненты
вычислительной системы: процессор, ОЗУ (RAM) и ПЗУ (ROM), графический процессор
(GPU), контроллеры периферии.Последовательность обработки данных организована 
следующим образом:

  1 Исходное изображение размером $28 \times 28$ пикселей поступает на вход 
сети через переходник интерфейсов AXI-uP и загружаются в память.

  2 Каждый пиксель передается в 28 MAC-ядер, которые выполняют умножение с 
соответствующими весовыми коэффициентами, извлекаемыми из памяти. Таким 
образом реализуется два этапа LST-1 обработки.

  3 Управляющее устройство синхронизирует процесс обработки, обновляя адреса 
памяти и координируя передачу данных между блоками.

  4 По завершении вычислений слоя LST-1 результирующее изображение передается 
на 10 MAC-ядер, где происходит окончательное формирование выходного вектора 
размерностью 10, каждый элемент которого соответствует весовому классу.

  5 Этот массив поступает в блок функции активации softmax, который выбирает 
класс с наибольшей вероятностью.

  6 Индекс класса передается на выход и отправляется в процессорную систему 
через uP и AXI4-lite интерфейсы. uP-интерфейс – это традиционный способ 
взаимодействия IP-блока с процессорной системой. Данный интерфейс 
предоставляет простую регистровую модель для обмена командами и 
данными\cite{chu_fpga}.

Основные сигналы uP-интерфейса:

  1 {ADDR} – адрес регистра, к которому выполняется обращение.

  2 {DATA\_IN} – входные данные, записываемые в регистр.

  3 {DATA\_OUT} – выходные данные, считываемые из регистра.

  4 {RD} – сигнал чтения из регистра.

  5 {WR} – сигнал записи в регистр.

  6 {CLK} – синхросигнал интерфейса.

  7 {RESET} – глобальный сброс интерфейса.

  8 {READY} – флаг готовности к передаче данных.

AXI4-Lite – это упрощённый вариант AXI4-интерфейса, предназначенный для 
управления и обмена небольшими порциями данных. В отличие от AXI4, он 
поддерживает только однослотовые транзакции без разделения на несколько 
пакетов\cite{vivado_axi}.

Основные сигналы AXI4-Lite:

    1 {AWADDR} – адрес записи.

    2 {AWVALID} – флаг валидности адреса записи.

    3 {WDATA} – записываемые данные.

    4 {WVALID} – сигнал валидности данных.

    5 {BREADY} – подтверждение завершения записи.

    6 {ARADDR} – адрес чтения.

    7 {ARVALID} – флаг валидности адреса чтения.

    8 {RREADY} – готовность принять данные.

    9 {RDATA} – считанные данные.

    10 {RVALID} – сигнал валидности данных.

    11 {ACLK} – системный тактовый сигнал.

    12 {ARESETN} – асинхронный сброс, активный по нулю.

Оба интерфейса широко применяются в проектировании цифровых систем. 
uP-интерфейс удобен для работы с простыми микроконтроллерами, тогда как 
AXI4-Lite предоставляет стандартизированный метод взаимодействия с 
процессорными системами, такими как ARM-ядра на FPGA.\@ При выборе 
интерфейса следует учитывать требуемую скорость обмена и сложность 
интеграции в систему.

Использование распределенной памяти и многопоточной обработки позволяет 
значительно ускорить вычисления, минимизировать задержки при выборке данных и 
повысить общую эффективность системы.

Электрическая структурная схема IP-блока нейронной сети прямого распространения 
приведена в приложении Г.